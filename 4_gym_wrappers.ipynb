{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/araffin/rl-tutorial-jnrr19/blob/master/4_gym_wrappers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "i8lIXBiHRYb6"
   },
   "source": [
    "# Stable Baselines Tutorial - Gym wrapper and hyperparameter tuning\n",
    "\n",
    "Github repo: https://github.com/araffin/rl-tutorial-jnrr19\n",
    "\n",
    "Stable-Baselines: https://github.com/hill-a/stable-baselines\n",
    "\n",
    "Documentation: https://stable-baselines.readthedocs.io/en/master/\n",
    "\n",
    "RL Baselines zoo: https://github.com/araffin/rl-baselines-zoo\n",
    "\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this notebook, you will learn how to use *Gym Wrappers* which allow to do monitoring, normalization, limit the number of steps, feature augmentation, ...\n",
    "\n",
    "\n",
    "\n",
    "You will also see that finding good hyperparameters is key to success in RL.\n",
    "\n",
    "## Install Dependencies and Stable Baselines Using Pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "owKXXp8rRZI7"
   },
   "outputs": [],
   "source": [
    "!apt install swig\n",
    "!pip install stable-baselines[mpi]==2.8.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "18ivrnsaSWUn"
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "from stable_baselines import A2C, SAC, PPO2, TD3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PytOtL9GdmrE"
   },
   "source": [
    "# The importance of hyperparameter tuning\n",
    "\n",
    "When compared with Supervised Learning, Deep Reinforcement Learning is far more sensitive to the choice of hyper-parameters such as learning rate, number of neurons, number of layers, optimizer ... etc. \n",
    "Poor choice of hyper-parameters can lead to poor/unstable convergence. This challenge is compounded by the variability in performance across random seeds (used to initialize the network weights and the environment).\n",
    "\n",
    "Here we demonstrate on a toy example the [Soft Actor Critic](https://arxiv.org/abs/1801.01290) algorithm applied in the Pendulum environment. Note the change in performance between the default and \"tuned\" parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "w5oVvYHwdnYv"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def evaluate(model, env, num_episodes=100):\n",
    "    # This function will only work for a single Environment\n",
    "    all_episode_rewards = []\n",
    "    for i in range(num_episodes):\n",
    "        episode_rewards = []\n",
    "        done = False\n",
    "        obs = env.reset()\n",
    "        while not done:\n",
    "            action, _states = model.predict(obs)\n",
    "            obs, reward, done, info = env.step(action)\n",
    "            episode_rewards.append(reward)\n",
    "\n",
    "        all_episode_rewards.append(sum(episode_rewards))\n",
    "\n",
    "    mean_episode_reward = np.mean(all_episode_rewards)\n",
    "    return mean_episode_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-a0v3fgwe54j"
   },
   "outputs": [],
   "source": [
    "eval_env = gym.make('Pendulum-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5WRR7kmIeqEB"
   },
   "outputs": [],
   "source": [
    "default_model = SAC('MlpPolicy', 'Pendulum-v0', verbose=1).learn(8000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "jQbDcbEheqWj",
    "outputId": "4558fe4f-5ed3-4a1e-d13c-91247936b817"
   },
   "outputs": [],
   "source": [
    "evaluate(default_model, eval_env, num_episodes=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "smMdkZnvfL1g"
   },
   "outputs": [],
   "source": [
    "tuned_model = SAC('MlpPolicy', 'Pendulum-v0', batch_size=256, verbose=1, policy_kwargs=dict(layers=[256, 256])).learn(8000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "DN05_Io8fMAr",
    "outputId": "163b8928-fb4b-483e-ea16-97f8becfc049"
   },
   "outputs": [],
   "source": [
    "evaluate(tuned_model, eval_env, num_episodes=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pi9IwxBYVMl8"
   },
   "source": [
    "Exploring hyperparameter tuning is out of the scope (and time schedule) of this tutorial. However, you need to know that we provide tuned hyperparameter in the [rl zoo](https://github.com/araffin/rl-baselines-zoo) as well as automatic hyperparameter optimization using [Optuna](https://github.com/pfnet/optuna).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hKwupU-Jgxjm"
   },
   "source": [
    "# Gym and VecEnv wrappers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ds4AAfmISQIA"
   },
   "source": [
    "## Anatomy of a gym wrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gnTS9e9hTzZZ"
   },
   "source": [
    "A gym wrapper follows the [gym](https://stable-baselines.readthedocs.io/en/master/guide/custom_env.html) interface: it has a `reset()` and `step()` method.\n",
    "\n",
    "Because a wrapper is *around* an environment, we can access it with `self.env`, this allow to easily interact with it without modifying the original env.\n",
    "There are many wrappers that have been predefined, for a complete list refer to [gym documentation](https://github.com/openai/gym/tree/master/gym/wrappers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hYo0C0TQSL3c"
   },
   "outputs": [],
   "source": [
    "class CustomWrapper(gym.Wrapper):\n",
    "  \"\"\"\n",
    "  :param env: (gym.Env) Gym environment that will be wrapped\n",
    "  \"\"\"\n",
    "  def __init__(self, env):\n",
    "    # Call the parent constructor, so we can access self.env later\n",
    "    super(CustomWrapper, self).__init__(env)\n",
    "  \n",
    "  def reset(self):\n",
    "    \"\"\"\n",
    "    Reset the environment \n",
    "    \"\"\"\n",
    "    obs = self.env.reset()\n",
    "    return obs\n",
    "\n",
    "  def step(self, action):\n",
    "    \"\"\"\n",
    "    :param action: ([float] or int) Action taken by the agent\n",
    "    :return: (np.ndarray, float, bool, dict) observation, reward, is the episode over?, additional informations\n",
    "    \"\"\"\n",
    "    obs, reward, done, info = self.env.step(action)\n",
    "    return obs, reward, done, info\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4zeGuyICUN26"
   },
   "source": [
    "## First example: limit the episode length\n",
    "\n",
    "One practical use case of a wrapper is when you want to limit the number of steps by episode, for that you will need to overwrite the `done` signal when the limit is reached. It is also a good practice to pass that information in the `info` dictionnary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Eb2U4_K6SNUx"
   },
   "outputs": [],
   "source": [
    "class TimeLimitWrapper(gym.Wrapper):\n",
    "  \"\"\"\n",
    "  :param env: (gym.Env) Gym environment that will be wrapped\n",
    "  :param max_steps: (int) Max number of steps per episode\n",
    "  \"\"\"\n",
    "  def __init__(self, env, max_steps=100):\n",
    "    # Call the parent constructor, so we can access self.env later\n",
    "    super(TimeLimitWrapper, self).__init__(env)\n",
    "    self.max_steps = max_steps\n",
    "    # Counter of steps per episode\n",
    "    self.current_step = 0\n",
    "  \n",
    "  def reset(self):\n",
    "    \"\"\"\n",
    "    Reset the environment \n",
    "    \"\"\"\n",
    "    # Reset the counter\n",
    "    self.current_step = 0\n",
    "    return self.env.reset()\n",
    "\n",
    "  def step(self, action):\n",
    "    \"\"\"\n",
    "    :param action: ([float] or int) Action taken by the agent\n",
    "    :return: (np.ndarray, float, bool, dict) observation, reward, is the episode over?, additional informations\n",
    "    \"\"\"\n",
    "    self.current_step += 1\n",
    "    obs, reward, done, info = self.env.step(action)\n",
    "    # Overwrite the done signal when \n",
    "    if self.current_step >= self.max_steps:\n",
    "      done = True\n",
    "      # Update the info dict to signal that the limit was exceeded\n",
    "      info['time_limit_reached'] = True\n",
    "    return obs, reward, done, info\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oZufaUJwVM9w"
   },
   "source": [
    "#### Test the wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "szZ43D5PVB07"
   },
   "outputs": [],
   "source": [
    "from gym.envs.classic_control.pendulum import PendulumEnv\n",
    "\n",
    "# Here we create the environment directly because gym.make() already wrap the environement in a TimeLimit wrapper otherwise\n",
    "env = PendulumEnv()\n",
    "# Wrap the environment\n",
    "env = TimeLimitWrapper(env, max_steps=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "cencka9iVg9V",
    "outputId": "366d9def-6e29-4974-fdcb-914dca5ff8a7"
   },
   "outputs": [],
   "source": [
    "obs = env.reset()\n",
    "done = False\n",
    "n_steps = 0\n",
    "while not done:\n",
    "  # Take random actions\n",
    "  random_action = env.action_space.sample()\n",
    "  obs, reward, done, info = env.step(random_action)\n",
    "  n_steps += 1\n",
    "\n",
    "print(n_steps, info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jkMYA63sV9aA"
   },
   "source": [
    "In practice, `gym` already have a wrapper for that named `TimeLimit` (`gym.wrappers.TimeLimit`) that is used by most environments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VIIJbSyQW9R-"
   },
   "source": [
    "## Second example: normalize actions\n",
    "\n",
    "It is usually a good idea to normalize observations and actions before giving it to the agent, this prevent [hard to debug issue](https://github.com/hill-a/stable-baselines/issues/473).\n",
    "\n",
    "In this example, we are going to normalize the action space of *Pendulum-v0* so it lies in [-1, 1] instead of [-2, 2].\n",
    "\n",
    "Note: here we are dealing with continuous actions, hence the `gym.Box` space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "F5E6kZfzW8vy"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class NormalizeActionWrapper(gym.Wrapper):\n",
    "  \"\"\"\n",
    "  :param env: (gym.Env) Gym environment that will be wrapped\n",
    "  \"\"\"\n",
    "  def __init__(self, env):\n",
    "    # Retrieve the action space\n",
    "    action_space = env.action_space\n",
    "    assert isinstance(action_space, gym.spaces.Box), \"This wrapper only works with continuous action space (spaces.Box)\"\n",
    "    # Retrieve the max/min values\n",
    "    self.low, self.high = action_space.low, action_space.high\n",
    "\n",
    "    # We modify the action space, so all actions will lie in [-1, 1]\n",
    "    env.action_space = gym.spaces.Box(low=-1, high=1, shape=action_space.shape, dtype=np.float32)\n",
    "\n",
    "    # Call the parent constructor, so we can access self.env later\n",
    "    super(NormalizeActionWrapper, self).__init__(env)\n",
    "  \n",
    "  def rescale_action(self, scaled_action):\n",
    "      \"\"\"\n",
    "      Rescale the action from [-1, 1] to [low, high]\n",
    "      (no need for symmetric action space)\n",
    "      :param scaled_action: (np.ndarray)\n",
    "      :return: (np.ndarray)\n",
    "      \"\"\"\n",
    "      return self.low + (0.5 * (scaled_action + 1.0) * (self.high -  self.low))\n",
    "\n",
    "  def reset(self):\n",
    "    \"\"\"\n",
    "    Reset the environment \n",
    "    \"\"\"\n",
    "    # Reset the counter\n",
    "    return self.env.reset()\n",
    "\n",
    "  def step(self, action):\n",
    "    \"\"\"\n",
    "    :param action: ([float] or int) Action taken by the agent\n",
    "    :return: (np.ndarray, float, bool, dict) observation, reward, is the episode over?, additional informations\n",
    "    \"\"\"\n",
    "    # Rescale action from [-1, 1] to original [low, high] interval\n",
    "    rescaled_action = self.rescale_action(action)\n",
    "    obs, reward, done, info = self.env.step(rescaled_action)\n",
    "    return obs, reward, done, info\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TmJ0eahNaR6K"
   },
   "source": [
    "#### Test before rescaling actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 213
    },
    "colab_type": "code",
    "id": "UEnjBwisaQIx",
    "outputId": "46b476fd-feb8-43a6-8dbf-3865142adf0d"
   },
   "outputs": [],
   "source": [
    "original_env = gym.make(\"Pendulum-v0\")\n",
    "\n",
    "print(original_env.action_space.low)\n",
    "for _ in range(10):\n",
    "  print(original_env.action_space.sample())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jvcll2L3afVd"
   },
   "source": [
    "#### Test the NormalizeAction wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 213
    },
    "colab_type": "code",
    "id": "WsCM9AUGaeBN",
    "outputId": "9d43ed0f-402b-4114-a453-78bfd731e98a"
   },
   "outputs": [],
   "source": [
    "env = NormalizeActionWrapper(gym.make(\"Pendulum-v0\"))\n",
    "\n",
    "print(env.action_space.low)\n",
    "\n",
    "for _ in range(10):\n",
    "  print(env.action_space.sample())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "V5h5kk2mbGNs"
   },
   "source": [
    "#### Test with a RL algorithm\n",
    "\n",
    "We are going to use the Monitor wrapper of stable baselines, wich allow to monitor training stats (mean episode reward, mean episode length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "R9FNCN8ybOVU"
   },
   "outputs": [],
   "source": [
    "from stable_baselines.bench import Monitor\n",
    "from stable_baselines.common.vec_env import DummyVecEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wutM3c1GbfGP"
   },
   "outputs": [],
   "source": [
    "env = Monitor(gym.make('Pendulum-v0'), filename=None, allow_early_resets=True)\n",
    "env = DummyVecEnv([lambda: env])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8cxnE5bdaQ_3"
   },
   "outputs": [],
   "source": [
    "model = A2C(\"MlpPolicy\", env, verbose=1).learn(int(1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EJFSM-Drb3Wc"
   },
   "source": [
    "With the action wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GszFZthob2wM"
   },
   "outputs": [],
   "source": [
    "normalized_env = Monitor(gym.make('Pendulum-v0'), filename=None, allow_early_resets=True)\n",
    "# Note that we can use multiple wrappers\n",
    "normalized_env = NormalizeActionWrapper(normalized_env)\n",
    "normalized_env = DummyVecEnv([lambda: normalized_env])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wrKJEO4NcIMd"
   },
   "outputs": [],
   "source": [
    "model_2 = A2C(\"MlpPolicy\", normalized_env, verbose=1).learn(int(1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5BxqXd_6dpJx"
   },
   "source": [
    "## Additional wrappers: VecEnvWrappers\n",
    "\n",
    "In the same vein as gym wrappers, stable baselines provide wrappers for `VecEnv`. Among the different that exist (and you can create your own), you should know: \n",
    "\n",
    "- VecNormalize: it computes a running mean and standard deviation to normalize observation and returns\n",
    "- VecFrameStack: it stacks several consecutive observations (useful to integrate time in the observation, e.g. sucessive frame of an atari game)\n",
    "\n",
    "More info in the [documentation](https://stable-baselines.readthedocs.io/en/master/guide/vec_envs.html#wrappers)\n",
    "\n",
    "Note: when using `VecNormalize` wrapper, you must save the running mean and std along with the model, otherwise you will not get proper results when loading the agent again. If you use the [rl zoo](https://github.com/araffin/rl-baselines-zoo), this is done automatically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zuIcbfv3g9dd"
   },
   "outputs": [],
   "source": [
    "from stable_baselines.common.vec_env import VecNormalize, VecFrameStack\n",
    "\n",
    "env = DummyVecEnv([lambda: gym.make(\"Pendulum-v0\")])\n",
    "normalized_vec_env = VecNormalize(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 195
    },
    "colab_type": "code",
    "id": "-PAbu21pg90A",
    "outputId": "6af7e8ed-0708-44b1-e29f-3456a32cb72f"
   },
   "outputs": [],
   "source": [
    "obs = normalized_vec_env.reset()\n",
    "for _ in range(10):\n",
    "  action = [normalized_vec_env.action_space.sample()]\n",
    "  obs, reward, _, _ = normalized_vec_env.step(action)\n",
    "  print(obs, reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UEpTys28Wz05"
   },
   "source": [
    "## Exercise: code you own monitor wrapper\n",
    "\n",
    "Now that you know how does a wrapper work and what you can do with it, it's time to experiment.\n",
    "\n",
    "The goal here is to create a wrapper that will monitor the training progress, storing both the episode reward (sum of reward for one episode) and episode length (number of steps in for the last episode).\n",
    "\n",
    "You will return those values using the `info` dict after each end of episode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8FWeDRd5W7hO"
   },
   "outputs": [],
   "source": [
    "class MyMonitorWrapper(gym.Wrapper):\n",
    "  \"\"\"\n",
    "  :param env: (gym.Env) Gym environment that will be wrapped\n",
    "  \"\"\"\n",
    "  def __init__(self, env):\n",
    "    # Call the parent constructor, so we can access self.env later\n",
    "    super(MyMonitorWrapper, self).__init__(env)\n",
    "    # === YOU CODE HERE ===#\n",
    "    # Initialize the variables that will be used\n",
    "    # to store the episode length and episode reward\n",
    "\n",
    "    # ====================== #\n",
    "  \n",
    "  def reset(self):\n",
    "    \"\"\"\n",
    "    Reset the environment \n",
    "    \"\"\"\n",
    "    obs = self.env.reset()\n",
    "    # === YOU CODE HERE ===#\n",
    "    # Reset the variables\n",
    "\n",
    "    # ====================== #\n",
    "    return obs\n",
    "\n",
    "  def step(self, action):\n",
    "    \"\"\"\n",
    "    :param action: ([float] or int) Action taken by the agent\n",
    "    :return: (np.ndarray, float, bool, dict) observation, reward, is the episode over?, additional informations\n",
    "    \"\"\"\n",
    "    obs, reward, done, info = self.env.step(action)\n",
    "    # === YOU CODE HERE ===#\n",
    "    # Update the current episode reward and episode length\n",
    "\n",
    "    # ====================== #\n",
    "\n",
    "    if done:\n",
    "      # === YOU CODE HERE ===#\n",
    "      # Store the episode length and episode reward in the info dict\n",
    "\n",
    "      # ====================== #\n",
    "    return obs, reward, done, info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "d4fY4QwWXNFK"
   },
   "source": [
    "#### Test your wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oWZp1olSXMUg"
   },
   "outputs": [],
   "source": [
    "env = gym.make(\"LunarLander-v2\")\n",
    "# === YOU CODE HERE ===#\n",
    "# Wrap the environment\n",
    "\n",
    "# Reset the environment\n",
    "\n",
    "# Take random actions in the enviromnent and check\n",
    "# that it returns the correct values after the end of each episode\n",
    "\n",
    "# ====================== #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6J-A3CsJXMgb"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xyotd2ZcZkcQ"
   },
   "source": [
    "## Conclusion\n",
    "\n",
    "\n",
    "In this notebook we have seen:\n",
    "- that good hyperparameters are key to the success of RL, you should not except the default ones to work on every problems\n",
    "- what is wrapper and what we can do with it\n",
    "- how to create your own wrapper\n",
    "\n",
    "\n",
    "Bonus exercise: combine your monitor wrapper with a callback to save the best according to the training performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qhWB_bHpSkas"
   },
   "source": [
    "## Wrapper Bonus: changing the observation space: a wrapper for episode of fixed length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bBlS9YxYSpJn"
   },
   "outputs": [],
   "source": [
    "from gym.wrappers import TimeLimit\n",
    "\n",
    "class TimeFeatureWrapper(gym.Wrapper):\n",
    "    \"\"\"\n",
    "    Add remaining time to observation space for fixed length episodes.\n",
    "    See https://arxiv.org/abs/1712.00378 and https://github.com/aravindr93/mjrl/issues/13.\n",
    "\n",
    "    :param env: (gym.Env)\n",
    "    :param max_steps: (int) Max number of steps of an episode\n",
    "        if it is not wrapped in a TimeLimit object.\n",
    "    :param test_mode: (bool) In test mode, the time feature is constant,\n",
    "        equal to zero. This allow to check that the agent did not overfit this feature,\n",
    "        learning a deterministic pre-defined sequence of actions.\n",
    "    \"\"\"\n",
    "    def __init__(self, env, max_steps=1000, test_mode=False):\n",
    "        assert isinstance(env.observation_space, gym.spaces.Box)\n",
    "        # Add a time feature to the observation\n",
    "        low, high = env.observation_space.low, env.observation_space.high\n",
    "        low, high= np.concatenate((low, [0])), np.concatenate((high, [1.]))\n",
    "        env.observation_space = gym.spaces.Box(low=low, high=high, dtype=np.float32)\n",
    "\n",
    "        super(TimeFeatureWrapper, self).__init__(env)\n",
    "\n",
    "        if isinstance(env, TimeLimit):\n",
    "            self._max_steps = env._max_episode_steps\n",
    "        else:\n",
    "            self._max_steps = max_steps\n",
    "        self._current_step = 0\n",
    "        self._test_mode = test_mode\n",
    "\n",
    "    def reset(self):\n",
    "        self._current_step = 0\n",
    "        return self._get_obs(self.env.reset())\n",
    "\n",
    "    def step(self, action):\n",
    "        self._current_step += 1\n",
    "        obs, reward, done, info = self.env.step(action)\n",
    "        return self._get_obs(obs), reward, done, info\n",
    "\n",
    "    def _get_obs(self, obs):\n",
    "        \"\"\"\n",
    "        Concatenate the time feature to the current observation.\n",
    "\n",
    "        :param obs: (np.ndarray)\n",
    "        :return: (np.ndarray)\n",
    "        \"\"\"\n",
    "        # Remaining time is more general\n",
    "        time_feature = 1 - (self._current_step / self._max_steps)\n",
    "        if self._test_mode:\n",
    "            time_feature = 1.0\n",
    "        # Optionnaly: concatenate [time_feature, time_feature ** 2]\n",
    "        return np.concatenate((obs, [time_feature]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "4_gym_wrappers.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

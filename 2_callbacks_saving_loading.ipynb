{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/araffin/rl-tutorial-jnrr19/blob/master/2_callbacks_saving_loading.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3ezJ3Y7XRUnj"
   },
   "source": [
    "# Stable Baselines Tutorial - Callbacks, saving models, and loading models\n",
    "\n",
    "Github repo: https://github.com/araffin/rl-tutorial-jnrr19\n",
    "\n",
    "Stable-Baselines: https://github.com/hill-a/stable-baselines\n",
    "\n",
    "Documentation: https://stable-baselines.readthedocs.io/en/master/\n",
    "\n",
    "RL Baselines zoo: https://github.com/araffin/rl-baselines-zoo\n",
    "\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this notebook, you will learn how to use *Callbacks* which allow to do monitoring, auto saving, model manipulation, progress bars, ...\n",
    "\n",
    "You will also see the *loading* and *saving* functions, and how to read the outputed files for possible exporting.\n",
    "\n",
    "## Install Dependencies and Stable Baselines Using Pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YFdlFByORUnl"
   },
   "outputs": [],
   "source": [
    "!apt install swig\n",
    "!pip install tqdm==4.36.1\n",
    "!pip install stable-baselines[mpi]==2.8.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "grXe85G9RUnp"
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "from stable_baselines import A2C, SAC, PPO2, TD3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mQRZlnhFRUns"
   },
   "source": [
    "## Helper functions\n",
    "This is to help the callbacks store variables (as they are function), but this could be also done by passing a class method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gdPSiMFzRUnt"
   },
   "outputs": [],
   "source": [
    "def get_callback_vars(model, **kwargs): \n",
    "    \"\"\"\n",
    "    Helps store variables for the callback functions\n",
    "    :param model: (BaseRLModel)\n",
    "    :param **kwargs: initial values of the callback variables\n",
    "    \"\"\"\n",
    "    # save the called attribute in the model\n",
    "    if not hasattr(model, \"_callback_vars\"): \n",
    "        model._callback_vars = dict(**kwargs)\n",
    "    else: # check all the kwargs are in the callback variables\n",
    "        for (name, val) in kwargs.items():\n",
    "            if name not in model._callback_vars:\n",
    "                model._callback_vars[name] = val\n",
    "    return model._callback_vars # return dict reference (mutable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving and loading\n",
    "\n",
    "Saving and loading stable-baselines models is straightforward: you can directly call `.save()` and `.load()` on the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Create save dir\n",
    "save_dir = \"/tmp/gym/\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "model = PPO2('MlpPolicy', 'Pendulum-v0', verbose=0).learn(8000)\n",
    "# The model will be saved under PPO2_tutorial.zip\n",
    "model.save(save_dir + \"/PPO2_tutorial\")\n",
    "\n",
    "# Check prediction before saving\n",
    "print(\"pre saved\", model.predict([0, 0, 0], deterministic=True))\n",
    "\n",
    "del model # delete trained model to demonstrate loading\n",
    "\n",
    "loaded_model = PPO2.load(save_dir + \"/PPO2_tutorial\")\n",
    "# Check that the prediction is the same after loading (for the same observation)\n",
    "print(\"loaded\", loaded_model.predict([0, 0, 0], deterministic=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving in stable-baselines is quite powerful, as you save the training hyperparameters, with the current weights. This means in practice, you can simply load a custom model, without redefining the parameters, and continue learning.\n",
    "\n",
    "The loading function can also update the model's class variables when loading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from stable_baselines.common.vec_env import DummyVecEnv\n",
    "\n",
    "# Create save dir\n",
    "save_dir = \"/tmp/gym/\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "model = A2C('MlpPolicy', 'Pendulum-v0', verbose=0, gamma=0.9, n_steps=20).learn(8000)\n",
    "# The model will be saved under A2C_tutorial.zip\n",
    "model.save(save_dir + \"/A2C_tutorial\")\n",
    "\n",
    "del model # delete trained model to demonstrate loading\n",
    "\n",
    "# load the model, and when loading set verbose to 1\n",
    "loaded_model = A2C.load(save_dir + \"/A2C_tutorial\", verbose=1)\n",
    "\n",
    "# show the save hyperparameters\n",
    "print(\"loaded:\", \"gamma =\", loaded_model.gamma, \"n_steps =\", loaded_model.n_steps)\n",
    "\n",
    "# as the environment is not serializable, we need to set a new instance of the environment\n",
    "loaded_model.set_env(DummyVecEnv([lambda: gym.make('Pendulum-v0')]))\n",
    "# and continue training\n",
    "loaded_model.learn(8000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "irHk8FXdRUnw"
   },
   "source": [
    "# Callbacks\n",
    "\n",
    "## A functional approach\n",
    "A callback function takes the `locals()` variables and the `globals()` variables from the model, then returns a boolean value for whether or not the training should continue.\n",
    "\n",
    "Thanks to the access to the models variables, in particular `_locals[\"self\"]`, we are able to even change the parameters of the model without halting the training, or changing the model's code.\n",
    "\n",
    "Here we have a simple callback that can only be called twice:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5gTXaNLARUnw",
    "outputId": "7683e8f6-079c-41b7-8a4c-1f89723b9223"
   },
   "outputs": [],
   "source": [
    "def simple_callback(_locals, _globals):\n",
    "    \"\"\"\n",
    "    Callback called at each step (for DQN an others) or after n steps (see ACER or PPO2)\n",
    "    :param _locals: (dict)\n",
    "    :param _globals: (dict)\n",
    "    \"\"\" \n",
    "    # get callback variables, with default values if unintialized\n",
    "    callback_vars = get_callback_vars(_locals[\"self\"], called=False) \n",
    "    \n",
    "    if not callback_vars[\"called\"]:\n",
    "        print(\"callback - first call\")\n",
    "        callback_vars[\"called\"] = True\n",
    "        return True # returns True, training continues.\n",
    "    else:\n",
    "        print(\"callback - second call\")\n",
    "        return False # returns False, training stops.\n",
    "\n",
    "model = SAC('MlpPolicy', 'Pendulum-v0', verbose=1)\n",
    "model.learn(8000, callback=simple_callback)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "adsKMvDkRUn0"
   },
   "source": [
    "## First example: Auto saving best model\n",
    "In RL, it is quite useful to keep a clean version of a model as you are training, as we can end up with burn-in of a bad policy. This is a typical use case for callback, as they can call the save function of the model, and observe the training over time.\n",
    "\n",
    "Using the monitoring wrapper, we can save statistics of the environment, and use them to determine the mean training reward.\n",
    "This allows us to save the best model while training.\n",
    "\n",
    "Note that this is not the proper way of evaluating an RL agent, you should create an test environment and evaluate the agent performance in the callback. For simplicity, we will be using the training reward as a proxy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1TuYLBEaRUn0"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from stable_baselines.bench import Monitor\n",
    "from stable_baselines.common.vec_env import DummyVecEnv\n",
    "from stable_baselines.results_plotter import load_results, ts2xy\n",
    "\n",
    "def auto_save_callback(_locals, _globals):\n",
    "    \"\"\"\n",
    "    Callback called at each step (for DQN an others) or after n steps (see ACER or PPO2)\n",
    "    :param _locals: (dict)\n",
    "    :param _globals: (dict)\n",
    "    \"\"\"\n",
    "    # get callback variables, with default values if unintialized\n",
    "    callback_vars = get_callback_vars(_locals[\"self\"], n_steps=0, best_mean_reward=-np.inf) \n",
    "\n",
    "    # skip every 20 steps\n",
    "    if callback_vars[\"n_steps\"] % 20 == 0:\n",
    "        # Evaluate policy training performance\n",
    "        x, y = ts2xy(load_results(log_dir), 'timesteps')\n",
    "        if len(x) > 0:\n",
    "            mean_reward = np.mean(y[-100:])\n",
    "\n",
    "            # New best model, you could save the agent here\n",
    "            if mean_reward > callback_vars[\"best_mean_reward\"]:\n",
    "                callback_vars[\"best_mean_reward\"] = mean_reward\n",
    "                # Example for saving best model\n",
    "                print(\"Saving new best model at {} timesteps\".format(x[-1]))\n",
    "                _locals['self'].save(log_dir + 'best_model')\n",
    "    callback_vars[\"n_steps\"] += 1\n",
    "    return True\n",
    "\n",
    "# Create log dir\n",
    "log_dir = \"/tmp/gym/\"\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "# Create and wrap the environment\n",
    "env = gym.make('CartPole-v1')\n",
    "env = Monitor(env, log_dir, allow_early_resets=True)\n",
    "env = DummyVecEnv([lambda: env])\n",
    "\n",
    "model = A2C('MlpPolicy', env, verbose=0)\n",
    "model.learn(total_timesteps=10000, callback=auto_save_callback)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Mx18FkEORUn3"
   },
   "source": [
    "## Second example: Realtime plotting of performance\n",
    "While training, it is sometimes useful to how the training progresses over time, relative to the episodic reward.\n",
    "For this, Stable-Baselines has [Tensorboard support](https://stable-baselines.readthedocs.io/en/master/guide/tensorboard.html), however this can be very combersome, especially in disk space usage. \n",
    "\n",
    "Here, we can use callback again, to plot the episodic reward in realtime, using the monitoring wrapper:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "c0Bu1HWKRUn4",
    "outputId": "785046b1-c7a5-43aa-fa72-268db1a9ae9e"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib notebook\n",
    "\n",
    "def plotting_callback(_locals, _globals):\n",
    "    \"\"\"\n",
    "    Callback called at each step (for DQN an others) or after n steps (see ACER or PPO2)\n",
    "    :param _locals: (dict)\n",
    "    :param _globals: (dict)\n",
    "    \"\"\"\n",
    "    # get callback variables, with default values if unintialized\n",
    "    callback_vars = get_callback_vars(_locals[\"self\"], plot=None) \n",
    "    \n",
    "    # get the monitor's data\n",
    "    x, y = ts2xy(load_results(log_dir), 'timesteps')\n",
    "    if callback_vars[\"plot\"] is None: # make the plot\n",
    "        plt.ion()\n",
    "        fig = plt.figure(figsize=(6,3))\n",
    "        ax = fig.add_subplot(111)\n",
    "        line, = ax.plot(x, y)\n",
    "        callback_vars[\"plot\"] = (line, ax, fig)\n",
    "        plt.show()\n",
    "    else: # update and rescale the plot\n",
    "        callback_vars[\"plot\"][0].set_data(x, y)\n",
    "        callback_vars[\"plot\"][-2].relim()\n",
    "        callback_vars[\"plot\"][-2].set_xlim([_locals[\"total_timesteps\"] * -0.02, \n",
    "                                            _locals[\"total_timesteps\"] * 1.02])\n",
    "        callback_vars[\"plot\"][-2].autoscale_view(True,True,True)\n",
    "        callback_vars[\"plot\"][-1].canvas.draw()\n",
    "        \n",
    "# Create log dir\n",
    "log_dir = \"/tmp/gym/\"\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "# Create and wrap the environment\n",
    "env = gym.make('MountainCarContinuous-v0')\n",
    "env = Monitor(env, log_dir, allow_early_resets=True)\n",
    "env = DummyVecEnv([lambda: env])\n",
    "        \n",
    "model = PPO2('MlpPolicy', env, verbose=0)\n",
    "model.learn(20000, callback=plotting_callback)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "49RVX7ieRUn7"
   },
   "source": [
    "## Third example: Progress bar\n",
    "Quality of life improvement are always welcome when developping and using RL. Here, we used [tqdm](https://tqdm.github.io/) to show a progress bar of the training, along with number of timesteps per second and the estimated time remaining to the end of the training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pXa8f6FsRUn8"
   },
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "# this callback uses the 'with' block, allowing for correct initialisation and destruction\n",
    "class progressbar_callback(object):\n",
    "    def __init__(self, total_timesteps): # init object with total timesteps\n",
    "        self.pbar = None\n",
    "        self.total_timesteps = total_timesteps\n",
    "        \n",
    "    def __enter__(self): # create the progress bar and callback, return the callback\n",
    "        self.pbar = tqdm(total=self.total_timesteps)\n",
    "        \n",
    "        def callback_progressbar(local_, global_):\n",
    "            self.pbar.n = local_[\"self\"].num_timesteps\n",
    "            self.pbar.update(0)\n",
    "            \n",
    "        return callback_progressbar\n",
    "\n",
    "    def __exit__(self, exc_type, exc_val, exc_tb): # close the callback\n",
    "        self.pbar.n = self.total_timesteps\n",
    "        self.pbar.update(0)\n",
    "        self.pbar.close()\n",
    "        \n",
    "model = TD3('MlpPolicy', 'Pendulum-v0', verbose=0)\n",
    "with progressbar_callback(2000) as callback: # this the garanties that the tqdm progress bar closes correctly\n",
    "    model.learn(2000, callback=callback)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lBF4ij46RUoC"
   },
   "source": [
    "## Forth example: Composition\n",
    "Thanks to the functional nature of callbacks, it is possible to do a composition of callbacks, into a single callback. This means we can auto save our best model, show the progess bar and episodic reward of the training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5hU3T9tkRUoD",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "def compose_callback(*callback_funcs): # takes a list of functions, and returns the composed function.\n",
    "    def _callback(_locals, _globals):\n",
    "        continue_training = True\n",
    "        for cb_func in callback_funcs:\n",
    "            if cb_func(_locals, _globals) is False: # as a callback can return None for legacy reasons.\n",
    "                continue_training = False\n",
    "        return continue_training\n",
    "    return _callback\n",
    "\n",
    "# Create log dir\n",
    "log_dir = \"/tmp/gym/\"\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "# Create and wrap the environment\n",
    "env = gym.make('CartPole-v1')\n",
    "env = Monitor(env, log_dir, allow_early_resets=True)\n",
    "env = DummyVecEnv([lambda: env])\n",
    "\n",
    "model = PPO2('MlpPolicy', env, verbose=0)\n",
    "with progressbar_callback(10000) as progress_callback:\n",
    "    model.learn(10000, callback=compose_callback(progress_callback, plotting_callback, auto_save_callback))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ojn4nvNNRUoT"
   },
   "source": [
    "## Going further - Saving format \n",
    "\n",
    "The format for saving and loading models has been recently revamped as of Stable-Baselines (>2.7.0).\n",
    "\n",
    "It is a zip-archived JSON dump and NumPy zip archive of the arrays:\n",
    "```\n",
    "saved_model.zip/\n",
    "├── data              JSON file of class-parameters (dictionary)\n",
    "├── parameter_list    JSON file of model parameters and their ordering (list)\n",
    "├── parameters        Bytes from numpy.savez (a zip file of the numpy arrays). ...\n",
    "    ├── ...           Being a zip-archive itself, this object can also be opened ...\n",
    "        ├── ...       as a zip-archive and browsed.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QWAcc8RFRUoU"
   },
   "source": [
    "## Save and find "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4tcQxzSCRUoV"
   },
   "outputs": [],
   "source": [
    "# Create save dir\n",
    "save_dir = \"/tmp/gym/\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "model = PPO2('MlpPolicy', 'Pendulum-v0', verbose=0).learn(8000)\n",
    "model.save(save_dir + \"/PPO2_tutorial\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rGaMNz4HRUoX",
    "outputId": "12dd5023-e35c-4c1b-8c51-7a9972cc2cbc"
   },
   "outputs": [],
   "source": [
    "!ls /tmp/gym/PPO2_tutorial*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gYY3nQyyRUoa",
    "outputId": "61f13df8-431b-43f9-e329-9ada09a9af83"
   },
   "outputs": [],
   "source": [
    "import zipfile\n",
    "\n",
    "archive = zipfile.ZipFile(\"/tmp/gym/PPO2_tutorial.zip\", 'r')\n",
    "for f in archive.filelist:\n",
    "  print(f.filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cPKkkTvjRUo2"
   },
   "source": [
    "## Exporting saved models\n",
    "\n",
    "And finally some futher reading for those who want to export to tensorflowJS or Java.\n",
    "\n",
    "https://stable-baselines.readthedocs.io/en/master/guide/export.html"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "2_callbacks_saving_loading.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
